{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Wic 베이스라인.ipynb","provenance":[],"mount_file_id":"1uJGCUCXSjQgdH5NhIaznilCCC04ft40d","authorship_tag":"ABX9TyMzBmGpkPdX9jze07arawfA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"RPHDtT7E7VO_","executionInfo":{"status":"ok","timestamp":1631339231258,"user_tz":-540,"elapsed":389,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"95ea1b9a-dc5e-4817-96eb-f8518d9ca388"},"source":["import pandas as pd\n","train= pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/국립국어원/NIKL_SKT_WiC_Train.tsv\", delimiter= '\\t')\n","test= pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/국립국어원/NIKL_SKT_WiC_Test.tsv\", delimiter= '\\t')\n","test.head()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Target</th>\n","      <th>SENTENCE1</th>\n","      <th>SENTENCE2</th>\n","      <th>ANSWER</th>\n","      <th>start_s1</th>\n","      <th>end_s1</th>\n","      <th>start_s2</th>\n","      <th>end_s2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>양분</td>\n","      <td>토양에 양분이 풍부하여 나무가 잘 자란다.</td>\n","      <td>태아는 모체로부터 양분과 산소를 공급받게 된다.</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>10</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>철</td>\n","      <td>철에 따라 피는 꽃.</td>\n","      <td>나는 손에 묻은 분필 가루를 털면서 철 늦은 오버를 치렁치렁 걸치고 걷는 교수님의 ...</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>풍경</td>\n","      <td>처마 끝에서 풍경이 울다.</td>\n","      <td>방 안 풍경을 둘러보다.</td>\n","      <td>NaN</td>\n","      <td>7</td>\n","      <td>9</td>\n","      <td>4</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>경사</td>\n","      <td>완만한 경사를 이룬 언덕.</td>\n","      <td>그 산은 경사가 급해서 오르기가 힘들다.</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>5</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>시</td>\n","      <td>시 한 편을 낭송하다.</td>\n","      <td>규칙을 어겼을 시에는 처벌을 받는다.</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID Target                SENTENCE1  ... end_s1  start_s2  end_s2\n","0   1     양분  토양에 양분이 풍부하여 나무가 잘 자란다.  ...      6        10      12\n","1   2      철              철에 따라 피는 꽃.  ...      1        20      21\n","2   3     풍경           처마 끝에서 풍경이 울다.  ...      9         4       6\n","3   4     경사           완만한 경사를 이룬 언덕.  ...      6         5       7\n","4   5      시             시 한 편을 낭송하다.  ...      1         8       9\n","\n","[5 rows x 9 columns]"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qcgRP4yn8ZbA","executionInfo":{"status":"ok","timestamp":1631339239273,"user_tz":-540,"elapsed":4954,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"ce1b84f3-9a06-48a7-f60f-7ce735c4cad8"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDZSqEpy7pNx","executionInfo":{"status":"ok","timestamp":1631339411673,"user_tz":-540,"elapsed":155326,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"0587d582-17d4-44bc-d08d-1b8f88e54fd0"},"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")\n","\n","model = AutoModelWithLMHead.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"FzcZsbRW8Elw","executionInfo":{"status":"ok","timestamp":1631339418995,"user_tz":-540,"elapsed":378,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["import torch"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65AAIfQ2_bRP","executionInfo":{"status":"ok","timestamp":1631340671861,"user_tz":-540,"elapsed":374,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"86ceffd0-6828-4f74-a699-1079146fa61d"},"source":["#######################gpt2 생성해보기 예제\n","\n","input_ids = tokenizer.encode(\"신기한데? 생각보다\")\n","input_ids\n"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[36645, 35943, 407, 39272]"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sY0pPSPyan2p","executionInfo":{"status":"ok","timestamp":1631340682155,"user_tz":-540,"elapsed":8803,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"6547e42a-f09e-4cd8-a647-3accc8f586e5"},"source":[" gen_ids = model.generate(torch.tensor([input_ids]),\n","                           max_length=20,\n","                           repetition_penalty=2.0,\n","                           pad_token_id=tokenizer.pad_token_id,\n","                           eos_token_id=tokenizer.eos_token_id,\n","                           bos_token_id=tokenizer.bos_token_id,\n","                           use_cache=True)\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)\n"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["신기한데? 생각보다 훨씬 더 맛있었어요! \n"," 그리고 또 하나!! 바로바로 이 팥\n"]}]},{"cell_type":"code","metadata":{"id":"HOuB-muianst"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfAtBYjl_hYv","executionInfo":{"status":"ok","timestamp":1631332624269,"user_tz":-540,"elapsed":368,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"2f01d9f5-a4ed-43c6-b806-bd3bffff84ea"},"source":["train_SENTENCE1_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in train.SENTENCE1]\n","train_SENTENCE2_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in train.SENTENCE2]\n","\n","train_SENTENCE1_bert[:5]"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 그의 죽음은 타살로 단정이 되었다. [SEP]',\n"," '[CLS] 현대 생활에서 단전과 단수의 고통은 겪어 보지 않으면 짐작도 못한다. [SEP]',\n"," '[CLS] 화성은 밤과 낮, 하루의 길이와 계절의 변화가 지구와 매우 비슷하다. [SEP]',\n"," '[CLS] 달의 자전 주기는 달이 지구의 둘레를 공전하는 주기와 같다. [SEP]',\n"," '[CLS] 오늘의 적이 내일은 동지가 될 수 있다. [SEP]']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwH9I3-uACNN","executionInfo":{"status":"ok","timestamp":1631332802596,"user_tz":-540,"elapsed":1284,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"cd9e7fb9-7594-4a6a-a88c-df4477a20632"},"source":["tokenized_train_SENTENCE1 = [tokenizer.tokenize(s) for s in train.SENTENCE1]\n","print(tokenized_train_SENTENCE1[0])"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["['▁그의', '▁죽음', '은', '▁타', '살', '로', '▁단', '정이', '▁되었다.']\n"]}]},{"cell_type":"code","metadata":{"id":"lq0DWKs3AOTW","executionInfo":{"status":"ok","timestamp":1631332859631,"user_tz":-540,"elapsed":1790,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["tokenized_train_SENTENCE2 = [tokenizer.tokenize(s) for s in train.SENTENCE2]\n","tokenized_test_SENTENCE1=[tokenizer.tokenize(s) for s in test.SENTENCE1]\n","tokenized_test_SENTENCE2= [tokenizer.tokenize(s) for s in test.SENTENCE2]"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-1Wxgo4gczk","executionInfo":{"status":"ok","timestamp":1631341119653,"user_tz":-540,"elapsed":6045,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"1a36bdfe-207d-4238-cb73-9f763d04a931"},"source":["!pip install sentencepiece\n"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388},"id":"YO-jNkSmC-b-","executionInfo":{"status":"error","timestamp":1631343082261,"user_tz":-540,"elapsed":363,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"b5512cc3-b335-4060-b9db-67f9ad989fa0"},"source":["##################################t5\n","  \n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from dataset import NIKL_SKT_WiC, T5_Classification_Collator\n","from torch.utils.data import DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","import torch\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score\n","\n","random_seed = 42\n","\n","random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","np.random.seed(random_seed)\n","torch.cuda.manual_seed(random_seed)\n","torch.cuda.manual_seed_all(random_seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","print(\"Load T5 model...\")\n","tokenizer = T5Tokenizer.from_pretrained(\"KETI-AIR/ke-t5-large\")\n","model = T5ForConditionalGeneration.from_pretrained(\"KETI-AIR/ke-t5-large\")\n","model.cuda()\n","\n","batch_size = 1\n","\n","collator = T5_Classification_Collator(use_tokenizer=tokenizer, max_sequence_len=128)\n","\n","print(\"Load Dataset...\")\n","train_dataset = NIKL_SKT_WiC('NIKL_SKT_WiC_Train.tsv')\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collator)\n","\n","dev_dataset = NIKL_SKT_WiC('NIKL_SKT_WiC_Dev.tsv')\n","dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n","\n","epochs = 10\n","total_steps = len(train_dataloader) * epochs\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # default is 1e-8.\n","                  )\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = len(train_dataloader), # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","print(\"Training...\")\n","\n","for epoch_i in range(epochs):\n","  print(\"-------------------------------------------------------------\")\n","  # train\n","  model.train()\n","\n","  true_labels = []\n","  prediction_labels = []\n","  total_loss = 0\n","\n","  for batch in tqdm(train_dataloader):\n","    true_labels += batch['labels'].numpy().flatten().tolist()\n","    batch = {k:v.type(torch.long).cuda() for k,v in batch.items()}\n","\n","    model.zero_grad()\n","    outputs = model(**batch)\n","\n","    loss, logits = outputs[:2]\n","    total_loss += loss.item()\n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    optimizer.step()\n","    scheduler.step()\n","\n","    logits = logits.detach().cpu().numpy()\n","\n","    prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n","\n","  train_avg_epoch_loss = total_loss / len(train_dataloader)\n","  train_acc = accuracy_score(true_labels, prediction_labels)\n","\n","  prediction_labels = []\n","  true_labels = []\n","  total_loss = 0\n","\n","  model.eval()\n","\n","  # Evaluate data for one epoch\n","  for batch in tqdm(dev_dataloader):\n","    true_labels += (batch['labels'].numpy())[:, 0].flatten().tolist()\n","    batch = {k:v.type(torch.long).cuda() for k,v in batch.items()}\n","\n","    with torch.no_grad():        \n","        outputs = model(**batch)\n","\n","        loss, logits = outputs[:2]\n","        \n","        logits = logits.detach().cpu().numpy()\n","        total_loss += loss.item()\n","\n","        predict_content = (logits.argmax(axis=-1))[:, 0].flatten().tolist()\n","        prediction_labels += predict_content\n","\n","  # Calculate the average loss over the training data.\n","  dev_avg_epoch_loss = total_loss / len(dev_dataloader)\n","  #print(len(true_labels))\n","  #print(len(prediction_labels))\n","  #print(len(dev_dataloader))\n","  dev_acc = accuracy_score(true_labels, prediction_labels)\n","\n","  print(\"Epoch: %d  train_loss: %.5f  train_acc: %.5f  dev_loss: %.5f  dev_acc: %.5f\"%(epoch_i, train_avg_epoch_loss, train_acc, dev_avg_epoch_loss, dev_acc))"],"execution_count":37,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-4b1f84068914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNIKL_SKT_WiC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5_Classification_Collator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"f4Feeg7QgUx2"},"source":[""],"execution_count":null,"outputs":[]}]}